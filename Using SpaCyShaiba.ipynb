{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f4c9d2",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "Building a resume parser using SpaCy can greatly streamline the process of extracting relevant information from resumes, enabling efficient candidate evaluation. In this guide, we will explore step-by-step instructions to develop a resume parser using the powerful natural language processing library, SpaCy.\n",
    "\n",
    "1. Understanding the Problem:\n",
    "Define the objective of the resume parser and the specific information to be extracted, such as name, contact details, skills, education, and work experience.\n",
    "\n",
    "2. Preparing the Environment:\n",
    "Install SpaCy and its required dependencies.\n",
    "Download and load the necessary SpaCy language models.\n",
    "\n",
    "3. Extracting Text from Resumes:\n",
    "Utilize PDF parsing libraries like pdfminer to extract text content from resume files.\n",
    "Implement a function to extract text from PDF files using the chosen library.\n",
    "\n",
    "4. Extracting Name from Resumes:\n",
    "Use SpaCy's linguistic capabilities to extract names from resume text.\n",
    "Define name patterns using SpaCy's Matcher module to identify different name formats.\n",
    "\n",
    "5. Extracting Contact Details:\n",
    "Employ regular expressions to extract contact numbers from resume text.\n",
    "Define patterns to capture various phone number formats.\n",
    "\n",
    "6. Extracting Email Addresses:\n",
    "Utilize regular expressions to identify and extract email addresses from resume text.\n",
    "Define email patterns to ensure accurate extraction.\n",
    "\n",
    "7. Extracting Skills:\n",
    "Create a predefined list of skills relevant to the desired job requirements.\n",
    "Utilize SpaCy's linguistic capabilities to match and extract skills from the resume text.\n",
    "\n",
    "8. Extracting Education:\n",
    "Define a set of education keywords or patterns to identify educational information.\n",
    "Utilize regular expressions to extract education details from the resume text.\n",
    "\n",
    "9. Putting it All Together:\n",
    "Combine the individual extraction functions to create a comprehensive resume parser.\n",
    "Process the resume text, extract the desired information, and store it in a structured format.\n",
    "\n",
    "10. Enhancements and Customizations:\n",
    "Explore advanced techniques to improve extraction accuracy, such as named entity recognition and entity linking.\n",
    "Consider handling different resume formats and languages for broader compatibility.\n",
    "Implement additional features like extracting work experience, certifications, or personal projects based on specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Note to Readers:\n",
    "\n",
    "I am thrilled to share with you a comprehensive guide on building a resume parser using SpaCy, which is now available on Analytics Vidhya. This guide aims to empower you with the knowledge and tools to create a powerful resume parser from scratch.\n",
    "\n",
    "Throughout the guide, I have covered various essential aspects of resume parsing, including text extraction from PDFs, extracting important information like contact details, skills, education, and more. I have also demonstrated how to leverage the capabilities of SpaCy, a popular natural language processing library, to perform these tasks effectively.\n",
    "\n",
    "By following the step-by-step instructions and code examples provided in the guide, you will gain a solid understanding of the resume parsing process and be equipped with the skills to build your own resume parser. Whether you are a data scientist, developer, or HR professional, this guide will help you streamline and automate the resume screening process, saving you valuable time and effort.\n",
    "\n",
    "I encourage you to dive into the guide, experiment with the code, and adapt it to your specific requirements. Remember that building a resume parser is an iterative process, and you may need to fine-tune and customize it based on the unique characteristics of the resumes you encounter.\n",
    "\n",
    "I would like to express my gratitude to the Analytics Vidhya platform for providing the opportunity to share this guide with the community. Their dedication to promoting knowledge sharing and empowering data professionals is truly commendable.\n",
    "\n",
    "I hope this guide serves as a valuable resource for you on your journey of building a resume parser using SpaCy. Feel free to reach out with any questions, feedback, or success stories you may have. Happy parsing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4004a3-058e-459c-bdaf-6403e09db9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (43.0.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\thinc\\types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\thinc\\compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py\", line 45, in <module>\n",
      "    from ._conv import register_converters as _register_converters, \\\n",
      "  File \"h5py\\\\_conv.pyx\", line 1, in init h5py._conv\n",
      "  File \"h5py\\\\h5r.pyx\", line 1, in init h5py.h5r\n",
      "  File \"h5py\\\\h5p.pyx\", line 1, in init h5py.h5p\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy pdfplumber\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6db6ef27-cc3b-4753-9602-d2303289d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber  # PDF text extraction\n",
    "import spacy        # NLP library\n",
    "import re           # Regex for email/phone/skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5199311f-f4ee-4b1e-aa4b-b09f0e0adaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Spacy loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703a7e37-fe5a-4b97-8996-657553f787b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1144f9-3dfb-4168-a675-2f9c07be1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return ent.text\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1e4a76-663a-4a60-acc6-70ea90b749e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email(text):\n",
    "    match = re.search(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", text)\n",
    "    return match.group() if match else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cba4ad3-76f3-44bf-aee6-b3912f4523fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phone(text):\n",
    "    match = re.search(r\"\\+?\\d[\\d\\s-]{8,}\\d\", text)\n",
    "    return match.group() if match else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3abb2c82-4bee-4edc-aa4b-bd42bf0487f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(text):\n",
    "    skills_list = ['Python', 'SQL', 'Power BI', 'Excel', 'Data Analysis', 'Machine Learning',\n",
    "                   'Deep Learning', 'Pandas', 'NumPy', 'Git', 'Jupyter', 'Canva']\n",
    "    found = []\n",
    "    for skill in skills_list:\n",
    "        if re.search(r'\\b' + re.escape(skill) + r'\\b', text, re.IGNORECASE):\n",
    "            found.append(skill)\n",
    "    return list(set(found))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9475714b-90ec-42b8-afdc-524092e41cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_education(text):\n",
    "    pattern = r\"(?i)(Bachelor|Master|B\\.Sc||MCA|Intermediate|High school)[^\\\\n]+\"\n",
    "    return re.findall(pattern, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2771d6ba-c998-4bab-953e-ef69260c1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name_custom(text):\n",
    "    lines = text.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'sa1610166@gmail.com' in line.lower():\n",
    "            # Return line just above email\n",
    "            for j in range(i-1, -1, -1):\n",
    "                name_candidate = lines[j].strip()\n",
    "                if name_candidate and all(x.isalpha() or x.isspace() for x in name_candidate):\n",
    "                    return name_candidate\n",
    "    return \"Name not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58d32a09-a7d0-482b-a40a-ef6a5254f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            if len(ent.text.split()) <= 4 and not any(char.isdigit() for char in ent.text):\n",
    "                return ent.text\n",
    "    return \"Name not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd7286b4-4624-4525-a00c-8b277cb2037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = extract_name_custom(text)\n",
    "if name == \"Name not found\":\n",
    "    name = extract_name_spacy(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "904896dd-29a8-484e-9d17-be4ee1d26f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¤ Name: SHAIBA ALI\n",
      "ðŸ“§ Email: sa1610166@gmail.com\n",
      "ðŸ“ž Phone: 7570919305\n",
      "ðŸ’¼ Skills: ['Python', 'Pandas', 'Excel', 'Data Analysis', 'Power BI', 'NumPy', 'Git', 'Canva', 'SQL', 'Jupyter']\n",
      "ðŸŽ“ Education: ['Master', 'Bachelor', 'Intermediate']\n"
     ]
    }
   ],
   "source": [
    "def extract_name(text):\n",
    "    \"\"\"\n",
    "    Extracts name from resume text by checking lines near email or using spaCy matcher.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Try using spaCy Matcher with common name patterns\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    name_patterns = [\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    ]\n",
    "    matcher.add(\"NAME\", patterns=name_patterns)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # Filter matched spans to avoid address-like or long results\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        name_candidate = span.text.strip()\n",
    "        if (len(name_candidate.split()) <= 3 and not any(char.isdigit() for char in name_candidate)):\n",
    "            return name_candidate\n",
    "\n",
    "    # Backup method: Try to extract name from lines near the email\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    email = extract_email_from_resume(text)\n",
    "    for i, line in enumerate(lines):\n",
    "        if email and email in line:\n",
    "            for j in range(i - 1, -1, -1):\n",
    "                candidate = lines[j]\n",
    "                if candidate and candidate.replace(\" \", \"\").isalpha():\n",
    "                    return candidate\n",
    "\n",
    "    return None\n",
    "\n",
    "resume_path = r\"C:\\Users\\dell\\Downloads\\resume.pdf\"  # Make sure file is in same folder\n",
    "\n",
    "text = extract_text_from_pdf(r\"C:\\Users\\dell\\Downloads\\resume.pdf\")\n",
    "\n",
    "print(\"ðŸ‘¤ Name:\", extract_name(text))\n",
    "print(\"ðŸ“§ Email:\", extract_email(text))\n",
    "print(\"ðŸ“ž Phone:\", extract_phone(text))\n",
    "print(\"ðŸ’¼ Skills:\", extract_skills(text))\n",
    "print(\"ðŸŽ“ Education:\", extract_education(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8017b-53d4-4298-a9fb-3b171d68d3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
